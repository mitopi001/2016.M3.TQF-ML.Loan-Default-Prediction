{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Loan Default Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import learning_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('/Users/johnmirror/TQF_Project_Data/train_v2.csv', sep=\",\", dtype=float, na_values='NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>...</th>\n",
       "      <th>f770</th>\n",
       "      <th>f771</th>\n",
       "      <th>f772</th>\n",
       "      <th>f773</th>\n",
       "      <th>f774</th>\n",
       "      <th>f775</th>\n",
       "      <th>f776</th>\n",
       "      <th>f777</th>\n",
       "      <th>f778</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.686842</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13699.0</td>\n",
       "      <td>7201.0</td>\n",
       "      <td>4949.0</td>\n",
       "      <td>126.75</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.14</td>\n",
       "      <td>-1.54</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.1833</td>\n",
       "      <td>0.7873</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.782776</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>84645.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>1625.0</td>\n",
       "      <td>123.52</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.1926</td>\n",
       "      <td>-0.6787</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.500080</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>83607.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>1527.0</td>\n",
       "      <td>127.76</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.89</td>\n",
       "      <td>-1.73</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.2521</td>\n",
       "      <td>0.7258</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.439874</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>82642.0</td>\n",
       "      <td>7542.0</td>\n",
       "      <td>1730.0</td>\n",
       "      <td>132.94</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.29</td>\n",
       "      <td>-0.89</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.2498</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.502749</td>\n",
       "      <td>2900.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>79124.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>491.0</td>\n",
       "      <td>122.72</td>\n",
       "      <td>...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>6.11</td>\n",
       "      <td>-3.82</td>\n",
       "      <td>2.51</td>\n",
       "      <td>0.2282</td>\n",
       "      <td>-0.5399</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 771 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>...</th>\n",
       "      <th>f770</th>\n",
       "      <th>f771</th>\n",
       "      <th>f772</th>\n",
       "      <th>f773</th>\n",
       "      <th>f774</th>\n",
       "      <th>f775</th>\n",
       "      <th>f776</th>\n",
       "      <th>f777</th>\n",
       "      <th>f778</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.686842</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13699.0</td>\n",
       "      <td>7201.0</td>\n",
       "      <td>4949.0</td>\n",
       "      <td>126.75</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.14</td>\n",
       "      <td>-1.54</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.1833</td>\n",
       "      <td>0.7873</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.782776</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>84645.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>1625.0</td>\n",
       "      <td>123.52</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.1926</td>\n",
       "      <td>-0.6787</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.500080</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>83607.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>1527.0</td>\n",
       "      <td>127.76</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.89</td>\n",
       "      <td>-1.73</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.2521</td>\n",
       "      <td>0.7258</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.439874</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>82642.0</td>\n",
       "      <td>7542.0</td>\n",
       "      <td>1730.0</td>\n",
       "      <td>132.94</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.29</td>\n",
       "      <td>-0.89</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.2498</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.502749</td>\n",
       "      <td>2900.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>79124.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>491.0</td>\n",
       "      <td>122.72</td>\n",
       "      <td>...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>6.11</td>\n",
       "      <td>-3.82</td>\n",
       "      <td>2.51</td>\n",
       "      <td>0.2282</td>\n",
       "      <td>-0.5399</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 771 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.shape[0]\n",
    "data_train['loss'].isnull().sum()  # count NAs in loss\n",
    "data_train['id'].isnull().sum()  # count NAs in ID\n",
    "(data_train['loss'] > 0).value_counts()  # count # of defaults\n",
    "sum(data_train.ix[:, 1:].duplicated());  # check for duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code to remove duplicate columns (all rows)\n",
    "remove = []\n",
    "cols = data_train.columns\n",
    "for i in range(len(cols) - 1):\n",
    "    v = data_train[cols[i]].values\n",
    "    for j in range(i + 1, len(cols)):\n",
    "        if np.array_equal(v, data_train[cols[j]].values):\n",
    "            remove.append(cols[j])\n",
    "            #print((cols[i], cols[j]))\n",
    "\n",
    "len(remove)  # number of duplicate pairs\n",
    "len(set(remove))  # number of columns to remove\n",
    "data_train = data_train.drop(remove, axis=1, inplace=False)\n",
    "\n",
    "## code to remove constant columns (all rows)\n",
    "remove = []\n",
    "for i in range(data_train.shape[1]):\n",
    "    if data_train.ix[:, i].nunique() == 1:\n",
    "        remove.append(cols[i])\n",
    "        #print(cols[i])\n",
    "\n",
    "len(set(remove))  # number of columns to remove\n",
    "data_train = data_train.drop(remove, axis=1, inplace=False)\n",
    "\n",
    "# data_train = data_train.sample(frac=1) # shuffling the rows - check: probably without rewriting (changing row numbers)\n",
    "# -> no need because I am dividing it randomly into training and testing sets?\n",
    "\n",
    "sum(data_train.isnull().sum())  # counts number of NAs in the dataset\n",
    "sum(data_train.isnull().sum()) / (data_train.shape[0] * (data_train.shape[1] - 1))\n",
    "data_train.shape[0] - data_train.dropna().shape[0]  # count number of rows without NA\n",
    "(data_train.shape[0] - data_train.dropna().shape[0]) / data_train.shape[0]\n",
    "## only 1% of all values are NA, but about 50% of rows contain at least one NA value\n",
    "np.count_nonzero(data_train.iloc[:, 1:-1].isnull().sum()) / data_train.iloc[:, 1:-1].shape[\n",
    "    1]  # 72% of features contain some NAs\n",
    "data_train.fillna(data_train.mean(), inplace=True);  # input missing values (with mean) -> OK (checked)\n",
    "\n",
    "## code to check for number of unique values\n",
    "uncols = []\n",
    "for i in range(data_train.shape[1]):\n",
    "    uncols.append(data_train.ix[:, i].nunique())\n",
    "\n",
    "uncols.sort()  # sort (ascending)\n",
    "\n",
    "## code to check for binary variables\n",
    "binary = []\n",
    "for i in range(data_train.shape[1]):\n",
    "    if (data_train.ix[:, i].nunique() == 2 and data_train.ix[:, i].min() == 0 and data_train.ix[:, i].max() == 1):\n",
    "        binary.append(cols[i])\n",
    "\n",
    "y = (data_train['loss'].values > 0)  # binary variable indicating if there was a default\n",
    "X = data_train.iloc[:, 1:-1]  # all features except ID and loss variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)  # train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.908\n"
     ]
    }
   ],
   "source": [
    "pipe_lr = Pipeline([('scl', StandardScaler()), ('clf', LogisticRegression(random_state=1))])\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "print('Test Accuracy: %.3f' % pipe_lr.score(X_test, y_test))\n",
    "y_pred = pipe_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALUAAAC1CAYAAAAZU76pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\neulrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110c3bb00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(2.5, 2.5))\n",
    "ax.matshow(confmat, cmap=plt.cm.BuPu, alpha=0.3)\n",
    "for i in range(confmat.shape[0]):\n",
    "    for j in range(confmat.shape[1]):\n",
    "        ax.text(x=j, y=i,\n",
    "                s=confmat[i, j],\n",
    "                va='center', ha='center')\n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('true label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curve L1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "computationally expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes, train_scores, test_scores = learning_curve(estimator=pipe_lr, X=X_train, y=y_train,\n",
    "                                                        train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "                                                        cv=10, n_jobs=1)\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label='training accuracy')\n",
    "plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='cyan')\n",
    "plt.plot(train_sizes, test_mean, color='red', linestyle='--', marker='s', markersize=5, label='validation accuracy')\n",
    "plt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std, alpha=0.15, color='red')\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel('Number of training samples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.908\n"
     ]
    }
   ],
   "source": [
    "pipe_lr = Pipeline([('scl', StandardScaler()), ('pca', PCA(n_components=2)), ('clf', LogisticRegression(random_state=1))])\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "print('Test Accuracy: %.3f' % pipe_lr.score(X_test, y_test))\n",
    "y_pred = pipe_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALUAAAC1CAYAAAAZU76pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1310817f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(2.5, 2.5))\n",
    "ax.matshow(confmat, cmap=plt.cm.BuPu, alpha=0.3)\n",
    "for i in range(confmat.shape[0]):\n",
    "    for j in range(confmat.shape[1]):\n",
    "        ax.text(x=j, y=i,\n",
    "                s=confmat[i, j],\n",
    "                va='center', ha='center')\n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('true label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "computationally expensive - killed after 30 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr = Pipeline([('scl', StandardScaler()), \n",
    "                    #('pca', PCA(n_components=2)),\n",
    "                    ('svm', SVC(kernel='rbf', random_state=0, gamma=0.20, C=5.0))])\n",
    "\n",
    "pipe_lr.fit(X_train, y_train)  # killed after 20 minutes\n",
    "print('Test Accuracy: %.3f' % pipe_lr.score(X_test, y_test))\n",
    "y_pred = pipe_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.any(np.isnan(X)) # ok\n",
    "np.all(np.isfinite(X)) # ok\n",
    "# -> must be value too large for float32\n",
    "X.values.max()\n",
    "np.finfo('float32').max\n",
    "# Note: Internally, its dtype will be converted to dtype=np.float32\n",
    "X.values.min() # ok\n",
    "X.columns[(sum(X.values>np.finfo('float32').max))>0]\n",
    "X.columns.get_loc('f391')\n",
    "X.columns.get_loc('f627')\n",
    "\n",
    "# drop the two columns\n",
    "del X['f391']\n",
    "del X['f627']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1) f67                            0.004184\n 2) f670                           0.003530\n 3) f3                             0.003162\n 4) f468                           0.003097\n 5) f471                           0.002975\n 6) f404                           0.002937\n 7) f655                           0.002933\n 8) f211                           0.002929\n 9) f766                           0.002907\n10) f746                           0.002860\n11) f596                           0.002857\n12) f598                           0.002849\n13) f432                           0.002789\n14) f212                           0.002678\n15) f271                           0.002655\n16) f629                           0.002647\n17) f412                           0.002641\n18) f75                            0.002637\n19) f384                           0.002626\n20) f282                           0.002620\n21) f76                            0.002590\n22) f393                           0.002535\n23) f405                           0.002521\n24) f640                           0.002519\n25) f514                           0.002515\n26) f674                           0.002514\n27) f630                           0.002489\n28) f281                           0.002485\n29) f767                           0.002482\n30) f533                           0.002467\n31) f654                           0.002455\n32) f638                           0.002451\n33) f739                           0.002442\n34) f132                           0.002425\n35) f142                           0.002420\n36) f775                           0.002407\n37) f652                           0.002400\n38) f366                           0.002392\n39) f614                           0.002388\n40) f672                           0.002385\n41) f422                           0.002381\n42) f601                           0.002379\n43) f673                           0.002375\n44) f433                           0.002367\n45) f333                           0.002349\n46) f143                           0.002348\n47) f738                           0.002335\n48) f734                           0.002328\n49) f737                           0.002326\n50) f16                            0.002304\n51) f413                           0.002303\n52) f646                           0.002296\n53) f13                            0.002285\n54) f377                           0.002260\n55) f289                           0.002249\n56) f201                           0.002243\n57) f740                           0.002232\n58) f68                            0.002228\n59) f639                           0.002227\n60) f518                           0.002227\n61) f743                           0.002202\n62) f647                           0.002196\n63) f431                           0.002195\n64) f149                           0.002189\n65) f63                            0.002184\n66) f637                           0.002178\n67) f64                            0.002161\n68) f442                           0.002159\n69) f45                            0.002156\n70) f272                           0.002156\n71) f332                           0.002155\n72) f71                            0.002141\n73) f374                           0.002141\n74) f54                            0.002141\n75) f180                           0.002140\n76) f597                           0.002138\n77) f516                           0.002137\n78) f613                           0.002137\n79) f406                           0.002135\n80) f19                            0.002131\n81) f376                           0.002130\n82) f56                            0.002130\n83) f774                           0.002118\n84) f600                           0.002107\n85) f609                           0.002101\n86) f522                           0.002100\n87) f46                            0.002099\n88) f6                             0.002090\n89) f133                           0.002084\n90) f402                           0.002082\n91) f26                            0.002082\n92) f682                           0.002078\n93) f696                           0.002073\n94) f44                            0.002071\n95) f727                           0.002068\n96) f680                           0.002065\n97) f179                           0.002063\n98) f144                           0.002056\n99) f59                            0.002051\n100) f322                           0.002044\n101) f525                           0.002043\n102) f261                           0.002037\n103) f202                           0.002036\n104) f60                            0.002035\n105) f32                            0.002030\n106) f768                           0.002004\n107) f671                           0.001995\n108) f756                           0.001991\n109) f658                           0.001988\n110) f70                            0.001988\n111) f659                           0.001988\n112) f733                           0.001987\n113) f55                            0.001985\n114) f10                            0.001984\n115) f448                           0.001978\n116) f536                           0.001977\n117) f765                           0.001974\n118) f278                           0.001973\n119) f716                           0.001966\n120) f170                           0.001964\n121) f645                           0.001964\n122) f357                           0.001964\n123) f451                           0.001963\n124) f612                           0.001961\n125) f436                           0.001957\n126) f745                           0.001956\n127) f385                           0.001950\n128) f458                           0.001946\n129) f349                           0.001943\n130) f365                           0.001939\n131) f279                           0.001936\n132) f444                           0.001935\n133) f395                           0.001931\n134) f392                           0.001922\n135) f676                           0.001921\n136) f398                           0.001921\n137) f628                           0.001920\n138) f636                           0.001918\n139) f367                           0.001917\n140) f148                           0.001917\n141) f499                           0.001915\n142) f9                             0.001910\n143) f280                           0.001910\n144) f169                           0.001910\n145) f641                           0.001908\n146) f209                           0.001907\n147) f251                           0.001904\n148) f509                           0.001902\n149) f122                           0.001900\n150) f218                           0.001893\n151) f150                           0.001891\n152) f160                           0.001890\n153) f479                           0.001881\n154) f664                           0.001879\n155) f588                           0.001875\n156) f273                           0.001873\n157) f159                           0.001872\n158) f657                           0.001872\n159) f631                           0.001869\n160) f635                           0.001866\n161) f22                            0.001857\n162) f524                           0.001850\n163) f399                           0.001850\n164) f401                           0.001846\n165) f517                           0.001844\n166) f23                            0.001843\n167) f751                           0.001842\n168) f642                           0.001841\n169) f21                            0.001839\n170) f66                            0.001839\n171) f443                           0.001838\n172) f425                           0.001835\n173) f69                            0.001832\n174) f360                           0.001829\n175) f147                           0.001828\n176) f378                           0.001825\n177) f359                           0.001824\n178) f50                            0.001819\n179) f373                           0.001816\n180) f25                            0.001815\n181) f660                           0.001814\n182) f375                           0.001814\n183) f288                           0.001813\n184) f39                            0.001810\n185) f763                           0.001806\n186) f191                           0.001805\n187) f624                           0.001801\n188) f663                           0.001801\n189) f49                            0.001797\n190) f208                           0.001796\n191) f340                           0.001794\n192) f369                           0.001794\n193) f351                           0.001793\n194) f513                           0.001791\n195) f648                           0.001789\n196) f662                           0.001788\n197) f387                           0.001785\n198) f623                           0.001785\n199) f397                           0.001783\n200) f489                           0.001782\n201) f388                           0.001781\n202) f441                           0.001781\n203) f368                           0.001780\n204) f57                            0.001778\n205) f210                           0.001777\n206) f760                           0.001777\n207) f203                           0.001777\n208) f396                           0.001767\n209) f146                           0.001765\n210) f352                           0.001762\n211) f620                           0.001760\n212) f643                           0.001760\n213) f383                           0.001757\n214) f611                           0.001755\n215) f669                           0.001751\n216) f622                           0.001751\n217) f61                            0.001746\n218) f361                           0.001745\n219) f141                           0.001744\n220) f199                           0.001739\n221) f43                            0.001737\n222) f634                           0.001736\n223) f20                            0.001733\n224) f40                            0.001729\n225) f744                           0.001728\n226) f778                           0.001723\n227) f47                            0.001723\n228) f65                            0.001722\n229) f424                           0.001721\n230) f342                           0.001716\n231) f370                           0.001715\n232) f450                           0.001714\n233) f189                           0.001713\n234) f139                           0.001710\n235) f213                           0.001708\n236) f53                            0.001705\n237) f15                            0.001702\n238) f386                           0.001700\n239) f434                           0.001689\n240) f14                            0.001685\n241) f134                           0.001681\n242) f145                           0.001679\n243) f430                           0.001679\n244) f344                           0.001678\n245) f339                           0.001677\n246) f350                           0.001667\n247) f140                           0.001664\n248) f343                           0.001664\n249) f51                            0.001659\n250) f358                           0.001653\n251) f435                           0.001652\n252) f41                            0.001649\n253) f423                           0.001646\n254) f651                           0.001642\n255) f726                           0.001637\n256) f334                           0.001635\n257) f112                           0.001634\n258) f353                           0.001633\n259) f190                           0.001632\n260) f356                           0.001629\n261) f618                           0.001618\n262) f77                            0.001617\n263) f72                            0.001614\n264) f621                           0.001609\n265) f348                           0.001604\n266) f17                            0.001604\n267) f650                           0.001602\n268) f31                            0.001596\n269) f661                           0.001576\n270) f277                           0.001574\n271) f593                           0.001572\n272) f330                           0.001568\n273) f181                           0.001563\n274) f29                            0.001562\n275) f314                           0.001560\n276) f81                            0.001552\n277) f18                            0.001551\n278) f411                           0.001550\n279) f283                           0.001545\n280) f668                           0.001540\n281) f587                           0.001539\n282) f207                           0.001538\n283) f653                           0.001530\n284) f200                           0.001528\n285) f656                           0.001526\n286) f341                           0.001523\n287) f30                            0.001520\n288) f217                           0.001513\n289) f429                           0.001512\n290) f221                           0.001509\n291) f80                            0.001500\n292) f677                           0.001496\n293) f230                           0.001489\n294) f742                           0.001489\n295) f205                           0.001488\n296) f206                           0.001485\n297) f338                           0.001483\n298) f102                           0.001481\n299) f394                           0.001471\n300) f229                           0.001471\n301) f204                           0.001466\n302) f626                           0.001465\n303) f161                           0.001461\n304) f219                           0.001457\n305) f390                           0.001456\n306) f410                           0.001455\n307) f526                           0.001444\n308) f337                           0.001443\n309) f287                           0.001437\n310) f220                           0.001429\n311) f1                             0.001426\n312) f138                           0.001421\n313) f586                           0.001421\n314) f619                           0.001420\n315) f698                           0.001409\n316) f667                           0.001408\n317) f695                           0.001400\n318) f428                           0.001389\n319) f382                           0.001388\n320) f416                           0.001387\n321) f735                           0.001382\n322) f336                           0.001382\n323) f523                           0.001370\n324) f193                           0.001369\n325) f276                           0.001359\n326) f91                            0.001357\n327) f409                           0.001353\n328) f331                           0.001349\n329) f171                           0.001344\n330) f240                           0.001338\n331) f250                           0.001335\n332) f665                           0.001335\n333) f286                           0.001332\n334) f101                           0.001328\n335) f231                           0.001328\n336) f90                            0.001327\n337) f92                            0.001324\n338) f381                           0.001320\n339) f298                           0.001320\n340) f632                           0.001320\n341) f275                           0.001315\n342) f421                           0.001305\n343) f111                           0.001304\n344) f216                           0.001303\n345) f715                           0.001295\n346) f183                           0.001292\n347) f364                           0.001289\n348) f589                           0.001288\n349) f178                           0.001282\n350) f249                           0.001282\n351) f520                           0.001275\n352) f239                           0.001274\n353) f732                           0.001273\n354) f594                           0.001273\n355) f260                           0.001270\n356) f110                           0.001269\n357) f440                           0.001265\n358) f163                           0.001261\n359) f24                            0.001259\n360) f100                           0.001256\n361) f137                           0.001253\n362) f173                           0.001253\n363) f694                           0.001253\n364) f697                           0.001247\n365) f28                            0.001246\n366) f546                           0.001245\n367) f717                           0.001243\n368) f259                           0.001235\n369) f335                           0.001232\n370) f285                           0.001228\n371) f27                            0.001226\n372) f168                           0.001223\n373) f773                           0.001223\n374) f415                           0.001223\n375) f308                           0.001214\n376) f755                           0.001212\n377) f610                           0.001212\n378) f306                           0.001210\n379) f136                           0.001209\n380) f380                           0.001209\n381) f414                           0.001208\n382) f407                           0.001207\n383) f324                           0.001207\n384) f153                           0.001205\n385) f400                           0.001204\n386) f566                           0.001204\n387) f363                           0.001204\n388) f420                           0.001203\n389) f269                           0.001199\n390) f772                           0.001191\n391) f426                           0.001188\n392) f731                           0.001186\n393) f419                           0.001182\n394) f754                           0.001178\n395) f158                           0.001176\n396) f771                           0.001171\n397) f590                           0.001167\n398) f241                           0.001167\n399) f418                           0.001161\n400) f82                            0.001157\n401) f300                           0.001151\n402) f464                           0.001151\n403) f699                           0.001148\n404) f135                           0.001148\n405) f195                           0.001146\n406) f449                           0.001145\n407) f151                           0.001145\n408) f194                           0.001136\n409) f79                            0.001134\n410) f438                           0.001131\n411) f721                           0.001125\n412) f197                           0.001124\n413) f470                           0.001121\n414) f196                           0.001121\n415) f316                           0.001121\n416) f769                           0.001121\n417) f730                           0.001120\n418) f270                           0.001118\n419) f519                           0.001113\n420) f753                           0.001112\n421) f121                           0.001111\n422) f215                           0.001109\n423) f607                           0.001107\n424) f556                           0.001104\n425) f446                           0.001104\n426) f689                           0.001092\n427) f675                           0.001086\n428) f469                           0.001086\n429) f313                           0.001084\n430) f693                           0.001080\n431) f186                           0.001080\n432) f347                           0.001075\n433) f720                           0.001068\n434) f472                           0.001066\n435) f437                           0.001065\n436) f120                           0.001063\n437) f535                           0.001060\n438) f466                           0.001059\n439) f747                           0.001057\n440) f529                           0.001057\n441) f445                           0.001055\n442) f187                           0.001054\n443) f4                             0.001052\n444) f185                           0.001050\n445) f728                           0.001049\n446) f184                           0.001047\n447) f606                           0.001047\n448) f710                           0.001045\n449) f372                           0.001041\n450) f130                           0.001040\n451) f78                            0.001034\n452) f346                           0.001031\n453) f750                           0.001025\n454) f692                           0.001024\n455) f719                           0.001023\n456) f188                           0.001021\n457) f527                           0.001018\n458) f214                           0.001018\n459) f712                           0.001018\n460) f305                           0.001018\n461) f263                           0.001018\n462) f274                           0.001016\n463) f253                           0.001015\n464) f465                           0.001013\n465) f705                           0.001010\n466) f761                           0.001010\n467) f528                           0.001008\n468) f545                           0.001007\n469) f762                           0.001007\n470) f131                           0.001000\n471) f323                           0.000993\n472) f684                           0.000991\n473) f752                           0.000990\n474) f759                           0.000989\n475) f714                           0.000988\n476) f177                           0.000986\n477) f355                           0.000982\n478) f711                           0.000981\n479) f228                           0.000980\n480) f709                           0.000980\n481) f165                           0.000980\n482) f591                           0.000980\n483) f175                           0.000980\n484) f749                           0.000976\n485) f157                           0.000975\n486) f167                           0.000974\n487) f537                           0.000974\n488) f704                           0.000973\n489) f718                           0.000971\n490) f534                           0.000968\n491) f688                           0.000968\n492) f706                           0.000967\n493) f511                           0.000965\n494) f748                           0.000964\n495) f174                           0.000964\n496) f505                           0.000961\n497) f691                           0.000958\n498) f176                           0.000956\n499) f166                           0.000955\n500) f284                           0.000954\n501) f292                           0.000951\n502) f540                           0.000949\n503) f154                           0.000947\n504) f5                             0.000947\n505) f503                           0.000942\n506) f592                           0.000940\n507) f758                           0.000939\n508) f531                           0.000937\n509) f156                           0.000932\n510) f757                           0.000932\n511) f707                           0.000930\n512) f454                           0.000930\n513) f164                           0.000929\n514) f515                           0.000927\n515) f223                           0.000924\n516) f155                           0.000921\n517) f708                           0.000919\n518) f447                           0.000919\n519) f290                           0.000917\n520) f687                           0.000915\n521) f488                           0.000915\n522) f460                           0.000912\n523) f2                             0.000912\n524) f498                           0.000910\n525) f506                           0.000910\n526) f508                           0.000905\n527) f608                           0.000905\n528) f7                             0.000905\n529) f457                           0.000904\n530) f512                           0.000904\n531) f565                           0.000904\n532) f501                           0.000904\n533) f683                           0.000902\n534) f521                           0.000898\n535) f560                           0.000896\n536) f467                           0.000892\n537) f507                           0.000891\n538) f543                           0.000890\n539) f510                           0.000890\n540) f532                           0.000890\n541) f538                           0.000889\n542) f563                           0.000888\n543) f573                           0.000887\n544) f504                           0.000884\n545) f725                           0.000881\n546) f685                           0.000880\n547) f530                           0.000880\n548) f258                           0.000880\n549) f553                           0.000877\n550) f690                           0.000877\n551) f439                           0.000875\n552) f8                             0.000874\n553) f491                           0.000873\n554) f108                           0.000870\n555) f297                           0.000868\n556) f87                            0.000868\n557) f539                           0.000868\n558) f555                           0.000866\n559) f575                           0.000865\n560) f452                           0.000863\n561) f562                           0.000862\n562) f582                           0.000861\n563) f686                           0.000861\n564) f561                           0.000860\n565) f485                           0.000860\n566) f105                           0.000859\n567) f453                           0.000859\n568) f495                           0.000858\n569) f124                           0.000853\n570) f456                           0.000853\n571) f107                           0.000852\n572) f496                           0.000850\n573) f564                           0.000850\n574) f547                           0.000849\n575) f476                           0.000848\n576) f481                           0.000847\n577) f558                           0.000847\n578) f570                           0.000846\n579) f227                           0.000845\n580) f106                           0.000844\n581) f542                           0.000844\n582) f226                           0.000843\n583) f85                            0.000843\n584) f88                            0.000839\n585) f544                           0.000838\n586) f567                           0.000836\n587) f497                           0.000835\n588) f459                           0.000835\n589) f461                           0.000833\n590) f238                           0.000830\n591) f86                            0.000828\n592) f114                           0.000827\n593) f455                           0.000826\n594) f475                           0.000826\n595) f477                           0.000824\n596) f486                           0.000824\n597) f225                           0.000824\n598) f252                           0.000824\n599) f224                           0.000821\n600) f493                           0.000819\n601) f321                           0.000818\n602) f198                           0.000817\n603) f262                           0.000817\n604) f403                           0.000816\n605) f550                           0.000816\n606) f502                           0.000816\n607) f89                            0.000811\n608) f116                           0.000810\n609) f115                           0.000809\n610) f500                           0.000808\n611) f579                           0.000804\n612) f483                           0.000804\n613) f482                           0.000804\n614) f117                           0.000804\n615) f554                           0.000803\n616) f584                           0.000800\n617) f557                           0.000799\n618) f484                           0.000799\n619) f244                           0.000799\n620) f492                           0.000797\n621) f126                           0.000796\n622) f552                           0.000796\n623) f103                           0.000793\n624) f551                           0.000791\n625) f487                           0.000790\n626) f580                           0.000789\n627) f246                           0.000789\n628) f494                           0.000789\n629) f123                           0.000788\n630) f541                           0.000787\n631) f480                           0.000787\n632) f247                           0.000786\n633) f127                           0.000785\n634) f248                           0.000784\n635) f548                           0.000784\n636) f125                           0.000783\n637) f490                           0.000783\n638) f571                           0.000780\n639) f581                           0.000779\n640) f118                           0.000778\n641) f559                           0.000778\n642) f128                           0.000777\n643) f568                           0.000776\n644) f95                            0.000775\n645) f96                            0.000775\n646) f569                           0.000774\n647) f97                            0.000773\n648) f576                           0.000769\n649) f245                           0.000768\n650) f254                           0.000767\n651) f222                           0.000766\n652) f572                           0.000764\n653) f99                            0.000762\n654) f256                           0.000759\n655) f583                           0.000758\n656) f255                           0.000757\n657) f585                           0.000756\n658) f578                           0.000756\n659) f257                           0.000755\n660) f98                            0.000754\n661) f577                           0.000752\n662) f264                           0.000752\n663) f192                           0.000749\n664) f265                           0.000749\n665) f549                           0.000744\n666) f113                           0.000744\n667) f266                           0.000743\n668) f574                           0.000738\n669) f267                           0.000735\n670) f237                           0.000735\n671) f236                           0.000733\n672) f235                           0.000732\n673) f119                           0.000729\n674) f315                           0.000726\n675) f109                           0.000723\n676) f268                           0.000722\n677) f234                           0.000719\n678) f329                           0.000719\n679) f84                            0.000718\n680) f162                           0.000711\n681) f233                           0.000711\n682) f679                           0.000708\n683) f243                           0.000700\n684) f666                           0.000691\n685) f94                            0.000690\n686) f182                           0.000677\n687) f93                            0.000655\n688) f309                           0.000646\n689) f172                           0.000629\n690) f232                           0.000628\n691) f299                           0.000621\n692) f617                           0.000604\n693) f104                           0.000599\n694) f129                           0.000598\n695) f242                           0.000593\n696) f325                           0.000589\n697) f48                            0.000579\n698) f83                            0.000577\n699) f301                           0.000570\n700) f307                           0.000569\n701) f317                           0.000564\n702) f152                           0.000546\n703) f62                            0.000543\n704) f625                           0.000528\n705) f42                            0.000528\n706) f633                           0.000524\n707) f389                           0.000523\n708) f713                           0.000519\n709) f74                            0.000518\n710) f615                           0.000517\n711) f681                           0.000513\n712) f36                            0.000513\n713) f722                           0.000508\n714) f770                           0.000508\n715) f52                            0.000505\n716) f58                            0.000504\n717) f644                           0.000504\n718) f293                           0.000502\n719) f73                            0.000477\n720) f604                           0.000417\n721) f616                           0.000375\n722) f291                           0.000362\n723) f595                           0.000323\n724) f776                           0.000166\n725) f723                           0.000105\n726) f777                           0.000038\n727) f724                           0.000032\n728) f736                           0.000000\n729) f678                           0.000000\n"
     ]
    }
   ],
   "source": [
    "feat_labels = X.columns\n",
    "forest = RandomForestClassifier(n_estimators=1000, random_state=0, n_jobs=-1)\n",
    "forest.fit(X, y)\n",
    "importances = forest.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    #print(\"%2d) %-*s %f\" % (f + 1, 30, feat_labels[f], importances[indices[f]]))\n",
    "# -> assigns the highest importance to the variables in order form first to last??? It always prints feat_labels in order\n",
    "#    from 0 to 31 regardless on any weights\n",
    "    print(\"%2d) %-*s %f\" % (f + 1, 30, feat_labels[indices[f]], importances[indices[f]])) # is it supposed to be like this?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105471, 50)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfm = SelectFromModel(forest,threshold=0.002303,prefit = True)\n",
    "X_sel = sfm.transform(X)\n",
    "X_sel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sel_train, X_sel_test, y_train, y_test = train_test_split(X_sel, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.908\n"
     ]
    }
   ],
   "source": [
    "pipe_lr = Pipeline([('scl', StandardScaler()), ('clf', LogisticRegression(random_state=1))])\n",
    "pipe_lr.fit(X_sel_train, y_train)\n",
    "print('Test Accuracy: %.3f' % pipe_lr.score(X_sel_test, y_test))\n",
    "y_pred = pipe_lr.predict(X_sel_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19153     2]\n [ 1934     6]]\n"
     ]
    }
   ],
   "source": [
    "confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "print(confmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not yet implemented"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}